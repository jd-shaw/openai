package com.shaw.openai.api.completion.chat;

import java.util.List;
import java.util.Map;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.NonNull;

/**
 * @author shaw
 * @date 2023/3/2
 *
 * https://platform.openai.com/docs/api-reference/chat/create
 */
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
@JsonInclude(JsonInclude.Include.NON_NULL)
public class ChatRequest {

	/**
	 *ID of the model to use. Currently, only gpt-3.5-turbo and gpt-3.5-turbo-0301 are supported.
	 */
	@NonNull
	private String model;

	/**
	 *The messages to generate chat completions for, in the chat format.
	 *
	 * messages=[
	 *         {"role": "system", "content": "You are a helpful assistant."},
	 *         {"role": "user", "content": "Who won the world series in 2020?"},
	 *         {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
	 *         {"role": "user", "content": "Where was it played?"}
	 *     ]
	 *
	 */
	@NonNull
	private List<ChatMessage> messages;

	/**
	 *使用什么样的采样温度，介于0和2之间。较高的值（如0.8）将使输出更加随机，而较低的值（例如0.2）将使其更加集中和确定。。
	 *
	 *我们通常建议使用this或 {@link ChatRequest#topP} ，但不能同时使用这两者。
	 */
	private Double temperature;

	/**
	 * Nucleus采样是一种替代使用温度参数的采样方式，模型会考虑概率分布在top_p中的所有token。例如，top_p=0.1表示只有组成前10%概率分布的token被考虑。
	 * 通常我们建议使用此参数或{@link ChatRequest#temperature}参数中的一个，而不是同时使用两个。
	 */
	@JsonProperty("top_p")
	private Double topP = 1d;

	/**
	 * 每个prompt生成的完成数。
	 * 由于此参数会生成大量的完成结果，因此可能会快速消耗您的token配额。请谨慎使用，并确保您合理设置了{@link ChatRequest#maxTokens}和{@link ChatRequest#stop}参数。
	 */
	private Integer n;

	/**
	 * 是否在部分完成时返回数据流。
	 * 如果设置了此参数，则仅当可用时，tokens将作为数据流事件发送，流将以数据: DONE消息终止。
	 */
	private Boolean stream = false;

	/**
	 * 最多4个序列，API将在这些序列后停止生成更多tokens。
	 * 返回的文本将不包含停止序列。
	 */
	private List<String> stop;

	/**
	 *要生成的最大令牌数。
	 *请求最多可以使用4096个在提示和完成之间共享的令牌。
	 *（普通英语文本的一个标记大约为4个字符）
	 */
	@JsonProperty("max_tokens")
	private Integer maxTokens;

	/**
	 * 0到1之间的数字（默认为0），根据新token在当前文本中的出现情况对其进行惩罚。
	 * 增加模型谈论新话题的可能性。
	 */
	@JsonProperty("presence_penalty")
	private Double presencePenalty = 0d;

	/**
	 * 0到1之间的数字（默认为0），根据新token在当前文本中的出现频率对其进行惩罚。
	 * 减少模型逐字重复同一句话的可能性。
	 */
	@JsonProperty("frequency_penalty")
	private Double frequencyPenalty = 0d;

	/**
	 * 在服务器端生成best_of个完成结果，并返回“最佳”（每个token的对数概率最低的完成结果）。
	 * 结果不能被实时返回。
	 * 当与{@link ChatRequest#n}一起使用时，best_of参数控制候选完成的数量，n参数指定要返回的数量，
	 best_of必须大于n。
	 */
	@JsonProperty("best_of")
	private Integer bestOf;

	/**
	 *
	* map Optional  Defaults to null
	* Modify the likelihood of specified tokens appearing in the completion.
	* Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically,
	 * the bias is added to the logits generated by the model prior to sampling.
	 * The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	 * values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	 */
	@JsonProperty("logit_bias")
	private Map<String, Integer> logitBias;

	/**
	 * 表示您的最终用户的唯一标识符，将帮助OpenAI监视和检测滥用行为。
	 */
	private String user;
}
